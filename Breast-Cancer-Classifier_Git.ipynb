{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cabed5-b57e-4da6-8e20-11abfb7e34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 1. Import Libraries\n",
    "# Import necessary libraries for data manipulation, numerical operations, and machine learning.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ### 2. Helper Function\n",
    "# Define a helper function to drop rows with missing values and reset the DataFrame index.\n",
    "def dropresetidx(df):\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.index += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef2bb7-60f8-4c46-98e4-dbb4d2b4f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 3. Load Data\n",
    "# Define the file path for the dataset and load it into a pandas DataFrame.\n",
    "file_path = (r'C:\\Users\\flavia\\Downloads\\cancerdiag\\wdbc.data')\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "\n",
    "# Apply the helper function to clean the DataFrame.\n",
    "df = dropresetidx(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10540c14-8b02-4445-9e4a-a3718821ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 4. Data Preparation\n",
    "# Define the column names based on the dataset's documentation.\n",
    "column_name = ['ID', 'Diagnosis']\n",
    "base_features = [\n",
    "    'radius', 'texture', 'perimeter', 'area', 'smoothness', \n",
    "    'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension'\n",
    "]\n",
    "for i in ['_mean', '_se', '_worst']:\n",
    "    for feature in base_features:\n",
    "        column_name.append(feature + i)\n",
    "\n",
    "# Assign the created list of names to the DataFrame's columns.\n",
    "df.columns = column_name\n",
    "\n",
    "# Convert the categorical 'Diagnosis' column to numerical format (Malignant: 1, Benign: 0).\n",
    "df['Diagnosis'].replace({'M': 1, 'B': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640d87e-b05f-44d9-80b8-3aa202695de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 5. Data Splitting\n",
    "# Split the dataset into training (80%) and testing (20%) sets to evaluate the model's performance on unseen data.\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "# ### 6. Feature and Target Definition\n",
    "# Define the list of feature columns and the target column.\n",
    "features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', \n",
    "            'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "            'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se',\n",
    "            'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
    "            'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "            'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
    "target = ['Diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012274d9-b5b0-42ac-863f-8b3a1fce4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 7. Feature Scaling\n",
    "# Scale the features using StandardScaler to ensure all features have a mean of 0 and a standard deviation of 1.\n",
    "# This helps the model converge faster and perform better.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[features]).T\n",
    "Y_train = train_df[target].values.T\n",
    "\n",
    "X_test = scaler.transform(test_df[features]).T\n",
    "Y_test = test_df[target].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7acec-0775-45de-ac69-372b3b12124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 8. Neural Network Implementation\n",
    "# Define the components of the neural network from scratch.\n",
    "\n",
    "# Define the size of the input, hidden, and output layers.\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]  # Number of features\n",
    "    n_h = 8           # Number of hidden units\n",
    "    n_y = Y.shape[0]  # Number of output units\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "# Initialize the model's parameters (weights and biases).\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "# Implement the forward propagation process.\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)  # Use tanh activation for the hidden layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) # Use sigmoid for the output layer for binary classification\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# Compute the cross-entropy cost function to measure the model's error.\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = -np.sum(logprobs) / m\n",
    "    return float(np.squeeze(cost))\n",
    "\n",
    "# Implement the backward propagation algorithm to calculate gradients.\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W2, A1, A2 = parameters['W2'], cache['A1'], cache['A2']\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) # Derivative of tanh\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "# Update the model's parameters using gradient descent.\n",
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    W1 = parameters['W1'] - learning_rate * grads['dW1']\n",
    "    b1 = parameters['b1'] - learning_rate * grads['db1']\n",
    "    W2 = parameters['W2'] - learning_rate * grads['dW2']\n",
    "    b2 = parameters['b2'] - learning_rate * grads['db2']\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "# Combine all functions into a single model training function.\n",
    "def nn_model(X, Y, n_h, num_iterations=10000, learning_rate=1.2, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x, _, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate) # Corrected this line\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost:.4f}\")\n",
    "            \n",
    "    return parameters\n",
    "\n",
    "# Define a function to make predictions on new data.\n",
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return (A2 > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683833a0-3fa6-4e54-9253-795717c4cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 9. Model Training\n",
    "# Train the neural network on the training data with specified hyperparameters.\n",
    "parameters = nn_model(X_train, Y_train, n_h=9, num_iterations=10000, learning_rate=0.1, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52004a28-4a6d-4d1f-ade6-28d60101700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 10. Model Evaluation\n",
    "# Make predictions on both the training and test sets.\n",
    "train_preds = predict(parameters, X_train)\n",
    "test_preds = predict(parameters, X_test)\n",
    "\n",
    "# Calculate and print the accuracy for both sets.\n",
    "train_acc = np.mean(train_preds == Y_train) * 100\n",
    "test_acc = np.mean(test_preds == Y_test) * 100\n",
    "print(f\"\\nTrain Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Print a detailed classification report for the test set.\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(Y_test.flatten(), test_preds.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99740102-c125-4806-92bd-e27a254ce3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 11. Prediction on Test DataFrame\n",
    "# Add the model's predictions as a new column to the test DataFrame for comparison.\n",
    "test_df['Pred_Label'] = test_preds.T.flatten()\n",
    "print(test_df[['Diagnosis','Pred_Label']])\n",
    "\n",
    "# Final accuracy check directly on the DataFrame.\n",
    "accuracy_test = (test_df['Diagnosis'] == test_df['Pred_Label']).mean()\n",
    "print(f'Final Accuracy Check: {accuracy_test:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
